{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac224c5",
   "metadata": {},
   "source": [
    "# Notebook for generating training data for NeRF from ROSBAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5472812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries for 3D visualization, point cloud manipulation, tensor operations\n",
    "from vedo import * # powerful visualization toolkit for 3D rendering (built on top of VTK)\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget # enables VTK 3D renderers to be shown inside Jupyter Notebooks\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 10*1024 # Set GPU memory limit to 10 GB\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# Load custom modules for spherical alignment and NeRF training\n",
    "from ICET_spherical import ICET # custom module for Iterative Closest Embedded Transform (used for aligning point clouds)\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as p\n",
    "from nerf_utils import *\n",
    "from coarse_network_utils import*\n",
    "\n",
    "# Enable IPython magic for automatic code reload and autosave\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b31a0",
   "metadata": {},
   "source": [
    "# Download dataset from external source\n",
    "\n",
    "ex: https://ori-drs.github.io/newer-college-dataset/\n",
    "\n",
    "Move to corresponding folder in ../data/\n",
    "\n",
    "\n",
    "# Export point clouds from ROSBAG to csv file  \n",
    "\n",
    "1. edit <bag2mapframe.py> to save generated point clouds to correct directory\n",
    "\n",
    "\n",
    "2. Move <bag2mapframe.py> to ros directory on your machine and run it with:\n",
    "\n",
    "```\n",
    "mv bag2mapframe.py ~/catkin_ws/src/bagconverter\n",
    "cd ~/catkin_ws\n",
    "catkin_make\n",
    "cd src/bagconverter\n",
    "roscore\n",
    "python3 bag2mapframe.py\n",
    "```\n",
    "3. play rosbag \n",
    "\n",
    "```\n",
    "rosbag play -r 0.1 myBag.bag\n",
    "```\n",
    "\n",
    "# Load and filter ground truth pose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process GT poses\n",
    "dir_name = \"~/PLINK/data/NewerCollegeDataset/\"\n",
    "experiment_name = \"01_short_experiment-20230331T172433Z-009/01_short_experiment/\"\n",
    "fn_gt = dir_name + experiment_name + \"ground_truth/registered_poses.csv\"\n",
    "\n",
    "# Load timestamped pose data from CSV\n",
    "# File format: sec, nsec, x, y, z, qx, qy, qz, qw\n",
    "gt = np.loadtxt(fn_gt, delimiter=',',skiprows = 1)\n",
    "\n",
    "# Extract individual components\n",
    "seconds = gt[:, 0] # Seconds part of timestamp\n",
    "nano_seconds = gt[:, 1] # Nanoseconds part of timestamp\n",
    "xyz = gt[:, 2:5] # Position: x, y, z\n",
    "qxyzw = gt[:, 5:] # Orientation: quaternion (qx, qy, qz, qw)\n",
    "\n",
    "# Total number of poses in the file\n",
    "num_poses = qxyzw.shape[0]\n",
    "\n",
    "# Convert each pose to a 4x4 transformation matrix\n",
    "# Create a batch of 4x4 identity matrices, one per pose\n",
    "sensor_poses = np.eye(4, dtype=np.float64).reshape(1, 4, 4).repeat(num_poses, axis=0)\n",
    "sensor_poses[:, :3, :3] = R.from_quat(qxyzw).as_matrix() # Fill in rotation from quaternion\n",
    "sensor_poses[:, :3, 3] = xyz # Fill in translation (position)\n",
    "\n",
    "# Transform poses from camera frame to LiDAR frame\n",
    "T_CL = np.eye(4, dtype=np.float32) # Transform from Camera to LiDAR (based on calibration)\n",
    "T_CL[:3, :3] = R.from_quat([0.0, 0.0, 0.924, 0.383]).as_matrix() # Rotation part (manually tuned / externally calibrated)\n",
    "T_CL[:3, 3] = np.array([-0.084, -0.025, 0.050], dtype=np.float32) # Translation part (manual offset between sensor origins)\n",
    "sensor_poses = np.einsum(\"nij,jk->nik\", sensor_poses, T_CL) # Apply T_CL to all poses using batch matrix multiplication\n",
    "\n",
    "# Normalize all poses relative to initial pose\n",
    "initial_pose = np.linalg.inv(sensor_poses[0]) # Save initial pose (inverse of first frame) to rebase all others\n",
    "poses_timestamps = seconds * 10e9 + nano_seconds # Compose timestamps (as int-like values) for future synchronization; scale seconds to nanoseconds\n",
    "sensor_poses = np.einsum(\"ij,njk->nik\", np.linalg.inv(sensor_poses[0]), sensor_poses) # Rebase all poses so that the first pose becomes the origin; TRY COMMENTING OUT...\n",
    "\n",
    "# Compute body-frame linear velocity (used for motion distortion correction)\n",
    "vel_world_frame = np.diff(sensor_poses[:,:3,-1], axis = 0) # Calculate velocity in world frame: difference in translation across time\n",
    "vel_body_frame = np.linalg.pinv(sensor_poses[1:,:3,:3]) @ vel_world_frame[:,:,None] # Convert world-frame velocity to body-frame using pseudo-inverse of rotation matrices\n",
    "vel_body_frame = vel_body_frame[:,:,0] # Remove singleton dimension\n",
    "\n",
    "# Smooth velocity using moving average to reduce noise\n",
    "# Helper function: compute simple moving average over window size `n`\n",
    "def moving_average(a, n=10):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "# Apply moving average smoothing (window=50) to each velocity component\n",
    "window=50\n",
    "MAx = moving_average(vel_body_frame[:,0], n = window)\n",
    "MAy = moving_average(vel_body_frame[:,1], n = window)\n",
    "MAz = moving_average(vel_body_frame[:,2], n = window)\n",
    "\n",
    "# Stack smoothed velocity components back together\n",
    "vel_body_frame = np.array([MAx, MAy, MAz]).T\n",
    "\n",
    "# Compute approximate angular velocity from Euler angle changes\n",
    "rot_vel_euls = np.diff(R.from_matrix(sensor_poses[:,:3,:3]).as_euler('xyz'), axis = 0) # Convert rotation matrices to Euler angles and take time-difference\n",
    "# Correct wrap-around issues: if angular diff > π, set to 0 (could refine with proper unwrapping)\n",
    "idx = np.argwhere(rot_vel_euls > (np.pi))  \n",
    "rot_vel_euls[idx] = 0\n",
    "idx = np.argwhere(rot_vel_euls < (-np.pi))\n",
    "rot_vel_euls[idx] = 0\n",
    "\n",
    "# Load HD map from .ply mesh\n",
    "# Courtyard section of Newer College Dataset (high-res 1 cm mesh)\n",
    "pl = '~/PLINK/data/NewerCollegeDataset/new-college-29-01-2020-1cm-resolution-1stSection - mesh.ply'\n",
    "\n",
    "# Alternate section (forest) — comment above line and uncomment below to switch\n",
    "# pl = '~/PLINK/data/NewerCollegeDataset/new-college-29-01-2020-1cm-resolution-5thSection.ply'\n",
    "\n",
    "# Load the mesh and extract only the vertex positions (i.e., 3D points)\n",
    "HD_map = trimesh.load(pl).vertices\n",
    "\n",
    "# Downsample HD map by keeping every N-th point (reduces memory & speeds up ICP)\n",
    "show_nth = 5\n",
    "submap = HD_map[::show_nth]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621380d0",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02883ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiDAR Image Generation Parameters\n",
    "n_images = 24 # Total number of LiDAR scans to process\n",
    "n_rots = 128 # Number of horizontal image patches covering 360°\n",
    "n_vert_patches = 1 # Vertical subdivisions between min and max vertical angles (phimin, phimax)\n",
    "useICET = True # Whether to use ICET (point cloud registration) for pose correction (turn off for some datasets like foliage)\n",
    "\n",
    "# Split each LiDAR image into (n_rots x n_vert_patches) patches via num pixels // num patches \n",
    "image_width = 1024//n_rots # Width of each image patch (pixels per horizontal division)\n",
    "image_height = 64//n_vert_patches # Height of each image patch (pixels per vertical division)\n",
    "shrink_factor = 0.005 # Scales the world to fit inside a compact coordinate system (used in courtyard mapping)\n",
    "\n",
    "n_cols_to_skip = n_rots // 8 # Mask out patches at edges of scans (e.g., where operator appears in frame)\n",
    "\n",
    "# Sensor-specific field of view calibration (Ouster OS1-64)\n",
    "phimin = np.deg2rad(-15.594) # Minimum vertical angle (in radians)\n",
    "phimax = np.deg2rad(17.743) # Maximum vertical angle (in radians)\n",
    "vert_fov = np.rad2deg(phimax-phimin) # Total vertical field of view in degrees\n",
    "\n",
    "# Allocate arrays for training outputs\n",
    "poses = np.zeros([n_images*n_rots*n_vert_patches,4,4]) # 4x4 camera pose for each patch (rotation + translation)\n",
    "\n",
    "# Each image contains two channels:\n",
    "#   - depth (channel 0)\n",
    "#   - raydrop mask (channel 1, 1 if valid, 0 if missing)\n",
    "images = np.ones([n_images*n_rots*n_vert_patches, 64//n_vert_patches, 1024//n_rots, 2])\n",
    "\n",
    "# [n total \"patches\", patch height, patch width, xyz]; ray origin and direction for each pixel (for NeRF ray marching)\n",
    "rays_o_all = np.zeros([n_images*n_rots*n_vert_patches, 64//n_vert_patches, 1024//n_rots, 3]) # origin of each ray in 3D space for each pixel in each patch\n",
    "rays_d_all = np.zeros([n_images*n_rots*n_vert_patches, 64//n_vert_patches, 1024//n_rots, 3]) # direction of each ray in 3D space for each pixel in each patch\n",
    "\n",
    "H, W = images.shape[1:3]\n",
    "\n",
    "redfix_hist = np.zeros([n_images,4,4]) # Save ICET correction transforms for each frame (applied to raw LiDAR)\n",
    "\n",
    "# Loop over LiDAR frames and extract ray/depth data\n",
    "for i in range(n_images):\n",
    "    print(i) \n",
    "    idx = i*50 + 7650  # Index into point cloud sequence (adjust depending on dataset split)\n",
    "    fn1 = \"~/PLINK/data/NewerCollegeDataset/01_Short_Experiment/point_clouds/frame_\" + str(idx) + \".npy\"\n",
    "    pc1 = np.load(fn1)\n",
    "\n",
    "    # Apply motion distortion correction using estimated velocity\n",
    "    # m_hat = estimated motion between frames: [dx, dy, dz, droll, dpitch, dyaw]\n",
    "    m_hat = np.array([-vel_body_frame[idx,0],\n",
    "                      -vel_body_frame[idx,1],\n",
    "                      -vel_body_frame[idx,2],\n",
    "#                       -rot_vel_euls[idx,0], (Optionally include angular correction from rot_vel_euls)\n",
    "#                       -rot_vel_euls[idx,1],\n",
    "#                       -rot_vel_euls[idx,2]\n",
    "                      0.,0.,0.\n",
    "                     ])   \n",
    "    pc1 = apply_motion_profile(pc1, m_hat, period_lidar=1.)  # Correct LiDAR sweep using motion profile\n",
    "    pc1 = np.flip(pc1, axis = 0) # Flip row ordering (sensor-specific correction)\n",
    "\n",
    "    # Register undistorted PC against HD Map using ICET to correct issues in ground truth\n",
    "    if useICET:\n",
    "        # Transform HD map points to LiDAR frame\n",
    "        submap_in_pc1_frame = (np.linalg.pinv(sensor_poses[idx]) @ initial_pose @ np.append(submap, np.ones([len(submap),1]), axis =1).T).T #test\n",
    "        submap_in_pc1_frame = submap_in_pc1_frame[:,:3] # Drop homogeneous column\n",
    "\n",
    "        # Use ICET to refine alignment of submap and scan\n",
    "        initial_guess = tf.constant([0.,0.,0.,0.,0.,0.])\n",
    "         # Run ICET algorithm for fine registration\n",
    "        it = ICET(cloud1 = submap_in_pc1_frame, cloud2 = pc1, fid = 50, niter = 8, \n",
    "           draw = False, group = 2, RM = False, DNN_filter = False, x0 = initial_guess)\n",
    "\n",
    "        # Transform original and corrected scans to map frame for inspection\n",
    "        pc1_in_map_frame = (initial_pose @ sensor_poses[idx] @ np.append(pc1, np.ones([len(pc1),1]), axis =1).T).T #test\n",
    "        pc1_in_map_frame = pc1_in_map_frame[:,:3]\n",
    "\n",
    "        pc1_corrected_in_map_frame = (initial_pose @ sensor_poses[idx] @ np.append(it.cloud2_tensor.numpy(), np.ones([len(it.cloud2_tensor.numpy()),1]), axis =1).T).T #test\n",
    "        pc1_corrected_in_map_frame = pc1_corrected_in_map_frame[:,:3]    \n",
    "\n",
    "        # Build correction transformation matrix from ICET output\n",
    "        redFix = np.eye(4)\n",
    "        redFix[:3,-1] = it.X[:3] # Apply translation from ICET\n",
    "        redFix[:3,:3] = redFix[:3,:3] @ R.from_euler('xyz', [it.X[3], it.X[4], it.X[5]]).as_matrix() # Apply rotation\n",
    "        redfix_hist[i] = redFix # Save correction matrix\n",
    "\n",
    "        # Apply ICET correction and transform back into LiDAR frame\n",
    "        redScanFixed = (redFix @ np.append(pc1, np.ones([len(pc1),1]), axis =1).T).T\n",
    "        redScanFixed = (sensor_poses[idx] @ np.append(redScanFixed[:,:3], np.ones([len(redScanFixed),1]), axis =1).T).T\n",
    " \n",
    "    else:\n",
    "        # No correction — just store identity matrix and original point cloud\n",
    "        redFix = np.eye(4)\n",
    "        redfix_hist[i] = redFix\n",
    "        redScanFixed = (redFix @ np.append(pc1, np.ones([len(pc1),1]), axis =1).T).T\n",
    "        redScanFixed = (sensor_poses[idx] @ np.append(redScanFixed[:,:3], np.ones([len(redScanFixed),1]), axis =1).T).T\n",
    "\n",
    "    # Convert corrected point cloud into spherical image\n",
    "    pc1_spherical = cartesian_to_spherical(pc1).numpy() # (r, theta, phi)\n",
    "    pcs = np.reshape(pc1_spherical, [-1,64,3]) # Reshape to match 64-row sensor: [n, 64, 3]\n",
    "    pcs = np.flip(pcs, axis = 1) # Flip vertically (sensor-specific)\n",
    "    raw_data = pcs[:,:,:] \n",
    "    raw_data = np.transpose(pcs, [1,0,2]) # Reorder axes to [rows, cols, features]\n",
    "\n",
    "    # Destagger depth images (OS1 unit has delay in sensor return bus)\n",
    "    data = np.zeros([64, 1024])\n",
    "    for k in range(np.shape(data)[0]//4):  # OS1 sends data in quartets\n",
    "        data[4*k,1:-8] = raw_data[4*k,9:,0]\n",
    "        data[4*k+1,1:-2] = raw_data[4*k+1,3:,0]\n",
    "        data[4*k+2,4:] = raw_data[4*k+2,:-4,0]\n",
    "        data[4*k+3,10:] = raw_data[4*k+3,:-10,0]\n",
    "    data = np.flip(data, axis =1) # Reverse horizontal order\n",
    "\n",
    "    # Generate rays (origin & direction) for this frame\n",
    "    rotm = sensor_poses[idx] @ redfix_hist[i]\n",
    "    rotm[0,-1] += 30\n",
    "    rotm[1,-1] += 30\n",
    "    rotm[2,-1] += 15 \n",
    "    rotm[:3,-1] *= shrink_factor #0.02 #0.005 #COURTYARD\n",
    "    #courtyard\n",
    "    rotm[0,-1] += 0.01 \n",
    "    rotm[1,-1] += 0.25 \n",
    "    rotm[2,-1] += 0.25 #translate above xy plane\n",
    "#     #forest\n",
    "#     rotm[0,-1] += 1.2 \n",
    "#     rotm[1,-1] += 1.25 \n",
    "#     rotm[2,-1] += 0.25 #translate above xy plane\n",
    "    ro, rd = get_rays_from_point_cloud(pc1, m_hat, rotm) \n",
    "\n",
    "    # Split full scan into ray-image patches\n",
    "    for j in range(n_rots):\n",
    "        for k in range(n_vert_patches):    \n",
    "            # Store ray directions\n",
    "            rd_in_patch = rd[k*image_height:(k+1)*image_height,j*image_width:(j+1)*image_width, :]\n",
    "            rays_d_all[k+(j+(i*n_rots))*n_vert_patches,:,:,:] = rd_in_patch\n",
    "\n",
    "            # Store ray origins\n",
    "            ro_in_patch = ro[k*image_height:(k+1)*image_height,j*image_width:(j+1)*image_width, :]\n",
    "            rays_o_all[k+(j+(i*n_rots))*n_vert_patches,:,:,:] = ro_in_patch            \n",
    "\n",
    "            # Store depth image for patch\n",
    "            # Crop vertically and horizontally\n",
    "            pcs = data[k*image_height:(k+1)*image_height,j*image_width:(j+1)*image_width] \n",
    "            #save depth information to first channel\n",
    "            images[k+(j+(i*n_rots))*n_vert_patches,:,:,0] = pcs\n",
    "            # Create raydrop mask (0 where invalid)\n",
    "            a = np.argwhere(abs(pcs) < 1)\n",
    "            images[k+(j+(i*n_rots))*n_vert_patches, a[:,0],a[:,1],1] = 0\n",
    "\n",
    "            # Get transformation matrix\n",
    "            # Centers origin at actual origin of HD map \n",
    "            # Update pose matrix for this patch\n",
    "            rotm = sensor_poses[idx] @ redfix_hist[i]\n",
    "\n",
    "            crop_angle = j*(2*np.pi/n_rots) - np.pi/2 + (np.pi/n_rots)\n",
    "            #account for the fact that sensor points back and to the left\n",
    "            rotm_crop = R.from_euler('xyz', [0,0,-crop_angle]).as_matrix() #test\n",
    "            rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "            rotm[0,-1] += 30\n",
    "            rotm[1,-1] += 30\n",
    "            rotm[2,-1] += 15 \n",
    "            rotm[:3,-1] *= shrink_factor\n",
    "            images[k+(j+(i*n_rots))*n_vert_patches,:,:,0] *= shrink_factor\n",
    "            #courtyard\n",
    "            rotm[0,-1] += 0.01 #shift up just a little\n",
    "            rotm[1,-1] += 0.25 #shift towards positive x\n",
    "            rotm[2,-1] += 0.25 #translate above xy plane\n",
    "#             #forest\n",
    "#             rotm[0,-1] += 1.2 \n",
    "#             rotm[1,-1] += 1.25 \n",
    "#             rotm[2,-1] += 0.25 #translate above xy plane\n",
    "\n",
    "            poses[k+(j+(i*n_rots))*n_vert_patches] = rotm \n",
    "\n",
    "# Remove patches where sensor is occluded by person holding lidar \n",
    "# Calculate how many columns of patches we need to skip at the beginning and end of each scan to avoid\n",
    "bad_idx = np.zeros([0,n_rots - 2*n_cols_to_skip]) # Create an empty array to store indices of patches that need to be removed\n",
    "a = np.linspace(0,n_rots*n_images*n_vert_patches-1,n_rots*n_images*n_vert_patches) # Create a linear array of all patch indices (total = n_images * n_rots * n_vert_patches)\n",
    "\n",
    "# Loop through the number of columns to skip on both ends\n",
    "for i in range(n_vert_patches*n_cols_to_skip):\n",
    "    bad_i_left = a[i::n_rots*n_vert_patches] # Identify patches in the i-th leftmost column across all scans\n",
    "    bad_idx = np.append(bad_idx, bad_i_left) # Append these indices to the list of bad indices\n",
    "    bad_i_right = a[(i+n_vert_patches*(n_rots-n_cols_to_skip))::n_rots*n_vert_patches] # Identify patches in the i-th rightmost column across all scans\n",
    "    bad_idx = np.append(bad_idx, bad_i_right) # Append these indices as well\n",
    "\n",
    "bad_idx = np.sort(bad_idx) # Sort the full list of bad patch indices in ascending order\n",
    "all_idx = np.linspace(0,n_rots*n_images*n_vert_patches-1,n_rots*n_images*n_vert_patches) # Create a full list of all patch indices\n",
    "good_idx = np.setdiff1d(all_idx, bad_idx).astype(int) # Subtract bad indices from full set to get valid (non-occluded) patch indices\n",
    "\n",
    "# Keep only valid (non-occluded) patches in each data array\n",
    "images = images[good_idx,:,:,:] \n",
    "poses = poses[good_idx,:,:]\n",
    "rays_d_all = rays_d_all[good_idx,:,:,:]\n",
    "rays_o_all = rays_o_all[good_idx,:,:,:]\n",
    "\n",
    "# Convert data types to float32 for compatibility with training frameworks\n",
    "images = images.astype(np.float32)\n",
    "poses = poses.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659654e",
   "metadata": {},
   "source": [
    "# Visualize training data\n",
    "\n",
    "draw training data in the same frame using depth images, ray origins (rays_o), \n",
    "and view directions (rays_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945be89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: draw rays and depth points from image patches\n",
    "def draw_frame_from_rays(disp, n_rots=128, n_vert_patches=1, frameIdx=0, \n",
    "                         color = 'red', stitched_map = np.zeros([0,3]) ):\n",
    "    \n",
    "    # Compute vertical resolution and elevation angle bins\n",
    "    phimin = np.deg2rad(-15.594) #took forever to figure this out...\n",
    "    phimax = np.deg2rad(17.743)\n",
    "    H = 64 // n_vert_patches\n",
    "    W = 1024 // n_rots\n",
    "    vertical_bins = np.linspace(phimin, phimax, n_vert_patches+1)  \n",
    "    phivals = np.linspace(phimin, phimax, 64) # Used for elevation slicing\n",
    "    n_cols_to_skip = n_rots // 8 # Mask first/last N columns\n",
    "\n",
    "    pts1 = np.zeros([1,3]) # Accumulator for all 3D points\n",
    "    # Loop over each patch for the selected frameIdx (excluding occluded columns)\n",
    "    for p in range(frameIdx*(n_rots - 2*n_cols_to_skip), (frameIdx + 1 )*(n_rots - 2*n_cols_to_skip)):\n",
    "        for i in range(n_vert_patches):\n",
    "            img_i = i\n",
    "            # Compute elevation range for the current vertical patch\n",
    "            idx_first=len(phivals) - (img_i%(n_vert_patches))*(64//n_vert_patches)-1 \n",
    "            idx_second= (len(phivals)- ((img_i+1)%(n_vert_patches))*(64//n_vert_patches))%len(phivals)\n",
    "            phimin_patch = phivals[idx_first]\n",
    "            phimax_patch = phivals[idx_second]\n",
    "\n",
    "            # Retrieve pose and rays for current patch\n",
    "            pose = poses[i + p*n_vert_patches]\n",
    "            rays_o = rays_o_all[i + p*n_vert_patches]\n",
    "            rays_d = rays_d_all[i + p*n_vert_patches]\n",
    "\n",
    "            # Reconstruct 3D points from rays and depth (projected in space)\n",
    "            inMap1 = add_patch(rays_o, rays_d, images[i+p*n_vert_patches,:,:,0])\n",
    "\n",
    "            # Accumulate all 3D points for display\n",
    "            pts1 = np.append(pts1, inMap1, axis = 0)\n",
    "        disp.append(Points(rays_o[0,:1,:], r = 15, c = 'purple')) # Debug: show ray origin as a purple sphere\n",
    "\n",
    "    # Render the projected 3D points (in assigned color, semi-transparent)\n",
    "    vizPts1 = Points(pts1, c = color, r = 3., alpha = 0.125)\n",
    "    disp.append(vizPts1)\n",
    "    \n",
    "    # Accumulate the 3D points into the stitched map (used across frames)\n",
    "    stitched_map = np.append(stitched_map, pts1, axis = 0)\n",
    "    return stitched_map\n",
    "    \n",
    "# Set up 3D plotter and visualize training set\n",
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[] # List of items to render\n",
    "\n",
    "# Allocate memory for accumulated 3D point cloud\n",
    "stitched_map = np.zeros([0,3])\n",
    "# Create a range of color intensities for different frames; colors = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'red']\n",
    "colors = np.linspace(0.1,0.3,n_images)[:,None] * np.array([[1,1,1]])\n",
    "# Draw all training frames one by one\n",
    "for i in range(len(colors)):\n",
    "    print(i)\n",
    "    stitched_map = draw_frame_from_rays(disp, n_rots = 128, n_vert_patches=1, \n",
    "                                        frameIdx = i, color=colors[i], stitched_map=stitched_map)\n",
    "\n",
    "# Display the visualization window\n",
    "plt.show(disp, \"Drawing training data from depth images, rays_o, and rays_d\")\n",
    "# Enable interactive 3D exploration in Jupyter Notebook\n",
    "ViewInteractiveWidget(plt.window)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823182c6",
   "metadata": {},
   "source": [
    "# Save training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"~/PLINK/data/NewerCollegeDataset/images.npy\", images)\n",
    "np.save(\"~/PLINK/data/NewerCollegeDataset/poses.npy\", poses)\n",
    "np.save(\"~/PLINK/data/NewerCollegeDataset/rays_o.npy\", rays_o_all)\n",
    "np.save(\"~/PLINK/data/NewerCollegeDataset/rays_d.npy\", rays_d_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
